{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-my-homepage","title":"Welcome to My Homepage!","text":""},{"location":"#my-name-is-haoxin-sang-hollis","title":"My name is Haoxin Sang (Hollis).","text":"<p>I'm excited to share my journey as a PhD student in CS at the University of Alberta. Welcome to my blog!</p> <p>Here, you'll find insights into my research, experiences as an international student, and reflections on various topics in Computer Science. (1)</p> <ol> <li>My content is written in English and, occasionally, in Chinese. I'm happy to exchange ideas in either language.</li> </ol>"},{"location":"#id-love-to-hear-your-thoughts-suggestions-or-just-have-a-chat-feel-free-to-reach-out-to-me-at-haoxinsanggmailcom-or-connect-with-me-on-discord","title":"I\u2019d love to hear your thoughts, suggestions, or just have a chat! Feel free to reach out to me at haoxinsang@gmail.com or connect with me on Discord.","text":""},{"location":"academic_profile/","title":"Academic Background","text":""},{"location":"academic_profile/#education","title":"Education","text":""},{"location":"academic_profile/#university-of-chinese-academy-of-sciences","title":"University of Chinese Academy of Sciences","text":"<pre><code>- M.S. in Fundamental Mathematics     Sep 2021 -- Jul 2024\n</code></pre>"},{"location":"academic_profile/#harbin-institute-of-technology","title":"Harbin Institute of Technology","text":"<pre><code>- B.S. in Mathematics and Applied Mathematics     Sep 2017 -- Jul 2021\n</code></pre>"},{"location":"academic_profile/#research-interest","title":"Research Interest","text":"<p>My research interests include online optimization and Riemannian optimization. I focus on online matching problems inspired by real-world applications such as real-time advertising, resource allocation in cloud computing, and matching users in ride-sharing platforms. Within online matching, I am particularly intrigued by stochastic models, including stochastic rewards and the Philosopher Inequalities.</p> <p>Additionally, I am passionate about Riemannian Optimization and learning approaches. This field leverages the geometric properties of Riemannian manifolds to develop algorithms that address challenges associated with non-Euclidean data.</p>"},{"location":"academic_profile/#publication","title":"Publication","text":"<ul> <li>Haoxin Sang, Yingyi Wu, Some analytic results and applications     in extremal Hermitian metrics*[J]. Journal of University of     Chinese Academy of Sciences, DOI:     10.7523/j.ucas.2023.091.</li> </ul>"},{"location":"academic_profile/#teaching-experience","title":"Teaching Experience","text":""},{"location":"academic_profile/#university-of-chinese-academy-of-sciences_1","title":"University of Chinese Academy of Sciences","text":"<pre><code>-   Teaching assistant for Differential Manifolds       Fall 2023   \n-   Teaching assistant for Calculus II, B       Spring 2023 \n-   Teaching assistant for Calculus I, B        Fall 2022\n</code></pre>"},{"location":"notes/naming_conventions/","title":"Naming Conventions in Python","text":"<p>Naming conventions are an important part of writing clean and readable code. In Python, the PEP 8 style guide provides recommendations on naming conventions. Here\u2019s a summary of the conventions along with special cases and additional tips:</p>"},{"location":"notes/naming_conventions/#naming-conventions","title":"Naming Conventions","text":"<ol> <li> <p>Variables:</p> </li> <li> <p>Use <code>snake_case</code> for variable names.</p> </li> <li>Example: <code>my_variable</code>, <code>total_count</code>, <code>is_valid</code>.</li> <li> <p>Functions:</p> </li> <li> <p>Use <code>snake_case</code> for function names.</p> </li> <li>Example: <code>def calculate_total()</code>, <code>def get_user_name()</code>.</li> <li> <p>Classes:</p> </li> <li> <p>Use <code>PascalCase</code> (also known as <code>CamelCase</code>).</p> </li> <li>Example: <code>class MyClass</code>, <code>class UserProfile</code>.</li> <li> <p>Modules:</p> </li> <li> <p>Use <code>snake_case</code> for module names (file names).</p> </li> <li>Example: <code>import my_module</code>, <code>from user_profile import get_user_name</code>.</li> <li> <p>Constants:</p> </li> <li> <p>Use <code>UPPER_CASE</code> for constants.</p> </li> <li>Example: <code>PI = 3.14159</code>, <code>MAX_CONNECTIONS = 10</code>.</li> </ol>"},{"location":"notes/naming_conventions/#special-cases","title":"Special Cases","text":"<ol> <li> <p>Abbreviations:</p> </li> <li> <p>Treat abbreviations as normal words and follow the same naming convention.</p> </li> <li>Variables and functions: <code>html_parser</code>, <code>parse_html</code>.</li> <li>Classes: <code>HtmlParser</code>.</li> <li> <p>Dashes (<code>-</code>):</p> </li> <li> <p>Dashes are not allowed in Python identifiers. Use underscores instead.</p> </li> <li>Example: <code>short_term</code> instead of <code>short-term</code>.</li> <li> <p>Names:</p> </li> <li> <p>Follow the same naming conventions for names. Treat them as any other word.</p> </li> <li>Example: <code>jack_smith</code> for a variable or function, <code>class JackSmith</code> for a class.</li> <li> <p>Numbers:</p> </li> <li> <p>Numbers can be included, but they should not start with a number.</p> </li> <li>Variables and functions: <code>user_123</code>, <code>find_25th_element</code>.</li> <li>Classes: <code>User123</code>, <code>Element25th</code>.</li> </ol>"},{"location":"notes/naming_conventions/#additional-tips","title":"Additional Tips","text":"<ol> <li> <p>Descriptive Names:</p> </li> <li> <p>Choose names that clearly describe the purpose of the variable, function, or class.</p> </li> <li>Avoid single-character names except for counters or iterators (e.g., <code>i</code>, <code>j</code>, <code>k</code>).</li> <li> <p>Avoid Ambiguity:</p> </li> <li> <p>Ensure that names are not ambiguous and convey the intended meaning.</p> </li> <li>Example: <code>calculate_total_price</code> is more descriptive than <code>calculate</code>.</li> <li> <p>Consistency:</p> </li> <li> <p>Be consistent with your naming conventions throughout your codebase.</p> </li> <li>Stick to the same style to improve readability and maintainability.</li> <li> <p>Avoid Using Reserved Keywords:</p> </li> <li> <p>Do not use Python reserved keywords as names for variables, functions, or classes.</p> </li> <li>Example: <code>class</code>, <code>def</code>, <code>return</code>, etc.</li> </ol>"},{"location":"notes/naming_conventions/#summary","title":"Summary","text":"<p>Here's a summary of the recommended naming conventions:</p> <ul> <li>Variables: <code>snake_case</code></li> <li>Functions: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Modules: <code>snake_case</code></li> <li>Constants: <code>UPPER_CASE</code></li> </ul>"},{"location":"notes/naming_conventions/#examples","title":"Examples","text":"<pre><code># Variables\nuser_name = \"JohnDoe\"\ntotal_count = 10\n\n# Functions\ndef calculate_total(price, tax):\n    return price + tax\n\ndef get_user_name():\n    return \"John Doe\"\n\n# Classes\nclass UserProfile:\n    def __init__(self, name):\n        self.name = name\n\n# Constants\nMAX_CONNECTIONS = 100\nPI = 3.14159\n\n# Modules (file names)\n# user_profile.py\n# html_parser.py\n\n# Example with abbreviations, names, and numbers\nhtml_parser = HtmlParser()\njack_smith = \"Jack Smith\"\nuser_123 = User123()\nfind_25th_element = find_25th_element()\n</code></pre>"},{"location":"notes/naming_conventions/#underscores","title":"Underscores","text":"<p>Underscores are used in Python for various special naming conventions. Understanding when and how to use underscores is important for writing idiomatic Python code. Here's a detailed explanation of the different uses of underscores in Python:</p>"},{"location":"notes/naming_conventions/#single-leading-underscore-_var","title":"Single Leading Underscore <code>_var</code>","text":"<ul> <li>Purpose: Indicates a weak \"internal use\" indicator. This is a convention to tell other programmers that the variable or method is intended for internal use. It does not prevent access but suggests that it should be treated as a non-public part of the API.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        self._internal_variable = 42\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#single-trailing-underscore-var_","title":"Single Trailing Underscore <code>var_</code>","text":"<ul> <li>Purpose: Used to avoid conflicts with Python keywords or built-in names.</li> <li>Example: <pre><code>def function_(parameter):\n    return parameter + 1\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#double-leading-underscore-__var","title":"Double Leading Underscore <code>__var</code>","text":"<ul> <li>Purpose: Triggers name mangling, where the interpreter changes the name of the variable in a way that makes it harder to create subclasses that accidentally override the private attributes and methods. This is used to avoid name conflicts in subclasses.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        self.__private_variable = 42\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#double-leading-and-trailing-underscore-var","title":"Double Leading and Trailing Underscore <code>__var__</code>","text":"<ul> <li>Purpose: Indicates special methods (also known as \"magic methods\" or \"dunder methods\") that have special meaning in Python. These are predefined methods used to perform operator overloading, object creation, and other fundamental behaviors.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        pass\n\n    def __str__(self):\n        return \"MyClass instance\"\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#single-underscore-_","title":"Single Underscore <code>_</code>","text":"<ul> <li>Purpose: Used as a throwaway variable name. This is a convention for variables that are temporary or insignificant.</li> <li>Example: <pre><code>for _ in range(5):\n    print(\"Hello, World!\")\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#references","title":"References","text":"<ol> <li> <p>PEP 8 - Style Guide for Python Code:</p> </li> <li> <p>The official Python style guide, PEP 8, covers naming conventions, formatting, and best practices for writing Python code.</p> </li> <li>Link: PEP 8 - Style Guide for Python Code</li> <li> <p>Python Documentation:</p> </li> <li> <p>The official Python documentation provides guidelines on naming conventions and special methods (also known as magic methods or dunder methods).</p> </li> <li>Link: Python Documentation</li> </ol>"},{"location":"notes/naming_conventions/#specific-sections-in-pep-8","title":"Specific Sections in PEP 8","text":"<ol> <li> <p>Naming Conventions:</p> </li> <li> <p>PEP 8 includes a section on naming conventions that describes how to name variables, functions, classes, constants, modules, and packages.</p> </li> <li>Link: PEP 8 - Naming Conventions</li> <li> <p>Method Names and Instance Variables:</p> </li> <li> <p>This section discusses conventions for naming methods and instance variables, including the use of single and double underscores.</p> </li> <li>Link: PEP 8 - Method Names and Instance Variables</li> <li> <p>Public and Internal Interfaces:</p> </li> <li> <p>Guidelines on how to distinguish between public and internal interfaces using naming conventions.</p> </li> <li>Link: PEP 8 - Public and Internal Interfaces</li> </ol>"},{"location":"notes/naming_conventions/#summary-of-naming-conventions-from-pep-8","title":"Summary of Naming Conventions from PEP 8","text":"<ul> <li> <p>Variables and Functions:</p> </li> <li> <p>Use <code>snake_case</code> for variables and function names.</p> </li> <li>Example: <code>my_variable</code>, <code>calculate_total</code>.</li> <li> <p>Classes:</p> </li> <li> <p>Use <code>PascalCase</code> for class names.</p> </li> <li>Example: <code>MyClass</code>, <code>UserProfile</code>.</li> <li> <p>Modules and Packages:</p> </li> <li> <p>Use <code>snake_case</code> for module and package names.</p> </li> <li>Example: <code>my_module</code>, <code>user_profile</code>.</li> <li> <p>Constants:</p> </li> <li> <p>Use <code>UPPER_CASE</code> for constants.</p> </li> <li>Example: <code>MAX_CONNECTIONS</code>, <code>PI</code>.</li> <li> <p>Private Variables and Methods:</p> </li> <li> <p>Use a single leading underscore <code>_</code> for weak internal use.</p> </li> <li>Example: <code>_internal_variable</code>, <code>_internal_method</code>.</li> <li> <p>Name Mangling:</p> </li> <li> <p>Use double leading underscores <code>__</code> to invoke name mangling.</p> </li> <li>Example: <code>__private_variable</code>, <code>__private_method</code>.</li> <li> <p>Special Methods:</p> </li> <li> <p>Use double leading and trailing underscores <code>__</code> for special methods.</p> </li> <li>Example: <code>__init__</code>, <code>__str__</code>.</li> <li> <p>Throwaway Variables:</p> </li> <li> <p>Use a single underscore <code>_</code> for throwaway variables.</p> </li> <li>Example: <code>for _ in range(10):</code>.</li> </ul>"},{"location":"notes/comparison_thm/comp_thm/","title":"Comparison Theorems in Riemannian Geometry","text":"<ul> <li>Jacobi field and conjugate points</li> <li>Rauch Comparison Theorem</li> <li>Morse index form and proof of Rauch comparison theorem</li> </ul>"},{"location":"notes/comparison_thm/comp_thm_app/","title":"Comparison Theorems in Riemannian Geometry and Applications","text":"<p>This note hasn't been carefully reviewed yet. </p> <ul> <li>Applications of Rauch Comparison Theorem</li> <li>Hessian Comparison Theorem and its application</li> <li>Toponogov\u2019s Theorem</li> </ul> <p></p>"},{"location":"notes/comparison_thm/preliminaries/","title":"Preliminaries to Comparison Theorems","text":"<ul> <li>Notation</li> <li>Induced covariant derivatives</li> </ul>"},{"location":"notes/hermitian_metric/readme/","title":"Hermitian Metric","text":"<ul> <li>The almost complex structure on vector spaces</li> <li>The fundamental form on vector spaces</li> <li>Complex and almost complex manifold</li> <li>Hermitian structure</li> </ul>"},{"location":"notes/laplacian_kahler_mfd/readme/","title":"The Laplacian Operator on Kahler Manifolds","text":""},{"location":"notes/lecture_notes/stat541_assignment1/","title":"Assignment 1","text":""},{"location":"notes/lecture_notes/stat541_week1/","title":"Week 1","text":""},{"location":"notes/lecture_notes/stat541_week1/#requirement","title":"Requirement","text":"<p>Statistics: Multiple Regression, Bias-var Decomposition</p> <p>Calculus: Interated Intigration</p>"},{"location":"notes/lecture_notes/stat541_week1/#supervised-learning","title":"Supervised Learning","text":"<p>Suppose we have data pairs</p> \\[ (x^{(1)},y^{(1)}),\\dots, (x^{(n)},y^{(n)}) \\sim P \\quad(iid.) \\] <p>where \\(P\\) is some distribution.</p>"},{"location":"notes/lecture_notes/stat541_week1/#our-goal","title":"Our Goal","text":"<p>Find a function \\(f:\\mathcal{X}\\rightarrow \\mathcal{Y}\\) such that \\(f(x)\\approx y\\) when \\((x,y)\\sim P\\), where</p> <ul> <li>\\(x\\) is called: predictor variable / covariates / independent var. / inputs / features</li> <li>\\(y\\) is called: response var. / output var. / dependent var.</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#examples","title":"Examples:","text":"<ul> <li>\\(\\mathcal{Y} = \\mathbb{R}\\): Regression Problem</li> <li>\\(\\mathcal{Y}\\) is a finite set: Image Classification</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#learning-algorithm","title":"Learning Algorithm","text":"<p>To obtain an \\(f\\) we use the training data to output on Learning algorithm</p> \\[ \\phi_n: (\\mathcal{X}\\times \\mathcal{Y})^n \\longrightarrow \\mathcal{Y}^\\mathcal{X} \\] <p>where \\(\\mathcal{Y}^\\mathcal{X}\\) is the set of all functions from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\). Therefore,</p> \\[ \\hat{f} = \\phi_n(x^{(1)},y^{(1)},\\dots, x^{(n)},y^{(n)}) \\] <p>is a random function(1) determined by the data and the learning algorithm \\(\\phi_n\\). </p> <ol> <li>Note that a random function is a deterministic function. More precisely, a function of an arbitrary argument \\(t\\) (defined on the set \\(T\\) of its values, and taking numerical values or, more generally, values in a vector space) whose values are defined in terms of a certain experiment and may vary with the outcome of this experiment according to a given probability distribution.</li> </ol>"},{"location":"notes/lecture_notes/stat541_week1/#definition-loss-function","title":"Definition Loss function","text":"<p>Loss function \\(L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow [0,+\\infty)\\) is to estimate the error of \\(f\\). How close \\(f(x)\\) is to \\(y\\) is gauged by \\(L(f(x),y)\\). Examples: squared error loss</p> \\[ L(f(x),y) = (y - f(x))^2,  \\] <p>and \\(0-1\\) loss</p> \\[ L(f(x),y) = I\\left(f(x) \\neq y\\right) = \\left\\{\\begin{matrix} 1,\\quad &amp; \\text{ if } f(x) \\neq y, \\\\ 0,\\quad &amp; \\text{ if } f(x) = y, \\end{matrix}\\right. \\] <p>where \\(I\\) is the indicator function. </p>"},{"location":"notes/lecture_notes/stat541_week1/#definition-risk-function","title":"Definition Risk function","text":"<ul> <li>Risk function (a.k.a. prediction error)</li> </ul> \\[ R(f,P) = E_{(x,y)}\\left(L(f(x),y)\\right) = \\int_{\\mathcal{X}\\times \\mathcal{Y}} L\\left(f(x),y\\right) p(x,y) \\,\\mathrm{d}x\\mathrm{d}y \\] <ul> <li>Oracle prediction error (a.k.a. Bayes Risk):</li> </ul> \\[ R^*(P) = \\inf_{f} R(f,P) \\] <ul> <li>Oracle predictor \\(f^*\\) satisfies</li> </ul> \\[ R(f^*,P) = R^*(P) \\]"},{"location":"notes/lecture_notes/stat541_week1/#compute-the-oracle-predictor","title":"Compute the Oracle Predictor","text":"<p>Compute \\(R^*(P)\\) for squared error loss:</p> \\[ \\begin{aligned} R(f,P)  &amp;= E_{(x,y)}\\left((f(x) - y)^2\\right) \\\\ &amp;= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} (y - f(x))^2 p(y|x) p(x) \\,\\mathrm{d}y \\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}}p(x)\\left(\\int_{\\mathcal{Y}}(y - f(x))^2 p(y|x)\\,\\mathrm{d}y\\right)\\,\\mathrm{d}x \\end{aligned} \\] <p>For fixed \\(x\\), we minimize over the value of \\(f(x)\\), that is, it's suffice to set</p> \\[ f(x) = \\operatorname*{arg\\, min}\\limits_z \\int_{\\mathcal{Y}}(y - z)^2 p(y|x)\\,\\mathrm{d}y. \\] <p>It is equivalent to minimize:</p> \\[ \\int_{\\mathcal{Y}} y^2 p(y|x)\\,\\mathrm{d}y - 2z\\cdot \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y + z^2\\cdot \\int_{\\mathcal{Y}} p(y|x)\\,\\mathrm{d}y. \\] <p>For the above equation, the first term is independent on \\(z\\) and</p> \\[ \\int_{\\mathcal{Y}} p(y|x)\\,\\mathrm{d}y = 1, \\] <p>for fixed \\(x\\). Then we have</p> \\[ \\begin{aligned} f(x) &amp;= \\operatorname*{arg\\, min}\\limits_{z} \\left(- 2z\\cdot \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y + z^2 \\right) \\\\ &amp;= \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y \\\\ &amp;= E(y|x).  \\end{aligned} \\] <p>Therefore, oracle predictor is given by \\(f^*(\\tilde{x})=E(y|X=\\tilde{x})\\). Similar arguments can be obtained for other loss functions. </p> <p>Additionally, our computation shows that making assumptions about the allowable \\(f\\), i.e. assume \\(f\\) lies in some set of functions \\(\\mathcal{F}\\), is somewhat equivalent to making assumptions about \\(P\\).</p>"},{"location":"notes/lecture_notes/stat541_week1/#making-assumptions","title":"Making Assumptions","text":"<p>Ideally we would like a \\(f\\) such that \\(R(f,P)\\) is small for all distribution \\(P\\). However, this is not possible by the No Free Lunch Theorem. Roughly, this says that for any \\(f\\), there exists a \\(P\\) such that \\(R(f,P)\\) is large. In classification, this says that there exists a \\(P\\) such that \\(f\\) is no better than random guessing(1). </p> <ol> <li>Random guessing is that we flip a coin and predict \\(y\\) based on the coin being heads or tails.</li> </ol> <p>Our solution is to make assumptions about \\(P\\):  </p> <p>Assume \\(P\\in\\mathcal{P}\\), where \\(\\mathcal{P}\\) is some subset of probability distributions. This suggest assumptions that we can make about the function class of predictors that we use. For example, \\(E(y|x)\\) is optimal for squared error loss. Given \\(\\mathcal{P}\\) we may want to restrict \\(\\mathcal{F}\\) to functions that have the form \\(E(y|x)\\) for \\(P\\in \\mathcal{P}\\).  </p>"},{"location":"notes/lecture_notes/stat541_week1/#complexity-bias-variance-tradeoff","title":"Complexity (bias-variance) tradeoff:","text":"<ul> <li>More complex function classes \\(\\mathcal{F}\\) -- low bias (i.e. able to approximate the oracle \\(E(y|x)\\))</li> <li>Large class of \\(\\mathcal{F}\\) -- it is hard to find the best \\(f\\).</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#error-decomposition","title":"Error Decomposition","text":"<p>For \\(f\\in \\mathcal{F}\\), since \\(\\mathcal{F}\\) may not be large enough, \\(R^*(P)\\) and \\(\\inf_{f\\in \\mathcal{F}} R(f,P)\\) may not be equal. We have the following decomposition of the risk function \\(R(f,P)\\):</p> \\[ {\\color{red} R(f,P) - \\inf_{f\\in \\mathcal{F}} R(f,P)} + {\\color{green} \\inf_{f\\in \\mathcal{F}} R(f,P) - R^*(P)} + {\\color{blue} R^*(P)}. \\] <ul> <li>\\({\\color{red} \\text{Red}}\\): Estimation error, which is non-negative.</li> <li>\\({\\color{green} \\text{Green}}\\): Approximation error, which is non-negative.</li> <li>\\({\\color{blue} \\text{Blue}}\\): the inherent error, which is the best we can do!</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#emperical-risk-minimization-erm","title":"Emperical Risk Minimization (ERM)","text":"<p>Idea is to find an approximation of \\(R(f,P)\\) and minimize this over \\(f\\) (also assume that \\(f\\) lies in some specified class of functions \\(\\mathcal{F}\\)). The ERM predictor \\(\\hat{f}\\) is defined as </p> \\[ \\hat{f} = \\operatorname*{arg\\, min}_{g\\in \\mathcal{F}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}L\\left(g(x^{(i)}),y^{(i)}\\right)\\right), \\] <p>which is called the average loss or emperical risk. Note that </p> <ul> <li>The emperical risk depends on the choice of function class \\(\\mathcal{F}\\), such as linear functions and complicated neural networks.</li> <li>Finding the <code>argmin</code> is not always easy, and hence there are various optimization algorithms approximating the <code>argmin</code>.</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#gauge-learning-algorithms","title":"Gauge Learning Algorithms","text":"<p>Given a learning algorithm \\(\\phi_n\\), how can we gauge the performance of \\(\\phi_n\\)? We can look at \\(R(\\hat{f},P)\\), that is, we view the training data \\(\\mathcal{D}_n\\) as random and then \\(R(\\hat{f},P)\\)(1) is a random variable. The expected risk (a.k.a. expected prediction error) is given by</p> <ol> <li>Here \\(\\hat{f}\\) is dependent on \\(\\mathcal{D}_n\\), which should be written as \\(\\hat{f}_{\\mathcal{D}_n}\\), but for simplicity, we still denote it as \\(\\hat{f}\\). </li> </ol> \\[ R(\\phi_n,P) = E_{\\mathcal{D}_n}\\left(R(\\hat{f},P)\\right) = E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(L(\\hat{f},y)\\right)\\right).  \\] <p>There are two sources of randomness: </p> <ul> <li>The training data \\(\\mathcal{D}_n\\) that determines \\(\\hat{f}\\). </li> <li>The pair \\((x,y)\\) where we predict \\(y\\) using \\(\\hat{f}(x)\\). </li> </ul>"},{"location":"notes/lecture_notes/stat541_week2/","title":"Week 2","text":""},{"location":"notes/lecture_notes/stat541_week2/#a-useful-technique","title":"A Useful Technique","text":"<p>Recall that the oracle predictor is given by</p> \\[ E(y|x) = \\int_{\\mathcal{Y}} y\\cdot p(y|x) \\,\\mathrm{d}y, \\] <p>which is a function of \\(x\\), denoted as \\(f^*(x)\\).</p>"},{"location":"notes/lecture_notes/stat541_week2/#key-property","title":"Key Property","text":"<p>We will prove the following equation</p> \\[ E_{(x,y)}\\left(g(x)\\cdot E(y|x)\\right) = E_{(x,y)}\\left(g(x)\\cdot y\\right), \\] <p>which holds for all functions \\(g\\) where \\(Var\\left(g(x)\\right)&lt;+\\infty\\)(1). Specifically, when taking \\(g(x)\\equiv 1\\), we obtain the law of total expectation, i.e. \\(E\\left(y\\right) = E\\left(E(y|x)\\right)\\)(2).</p> <ol> <li>This a technical assumption which is usually satisfied in practical problems.</li> <li>One special case states that if \\(\\left\\{A_i\\right\\}\\) is a finite or countable partition of the sample space, then \\(E(X)=\\sum_i E\\left(X | A_i\\right) \\cdot Pr\\left(A_i\\right)\\).</li> </ol> <p>Obviously, it is equivalent to</p> \\[ E_{(x,y)}\\left(g(x)\\left(y-E(y|x)\\right)\\right) = 0, \\] <p>which is an orthogonality property.</p>"},{"location":"notes/lecture_notes/stat541_week2/#proof","title":"Proof","text":"\\[ \\begin{aligned} E_(x,y)\\left(g(x)\\cdot E(y|x)\\right)  &amp;= E_x\\left(g(x)\\cdot E(y|x)\\right) \\\\ &amp;= \\int_{\\mathcal{X}}g(x)\\cdot E(y|x) p(x) \\,\\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}}g(x)\\cdot p(x) \\int_{\\mathcal{Y}} y\\cdot p(y|x) \\,\\mathrm{d}y \\,\\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} g(x)\\cdot y \\cdot p(y|x) p(x) \\,\\mathrm{d}y \\,\\mathrm{d}x \\\\ &amp;= E_{(x,y)}\\left(g(x)\\cdot y\\right), \\end{aligned} \\] <p>where the last equation is from \\(p(y|x) p(x) = p(x,y)\\).</p>"},{"location":"notes/lecture_notes/stat541_week2/#reprove-the-oracle-predictor","title":"Reprove the Oracle Predictor","text":"<p>Let us show again that \\(E(y|x)\\) is the oracle predictor under the squared error loss:</p> \\[ \\begin{aligned} R(f,P)  &amp;= E_{(x,y)}\\left(\\left(f(x)-y\\right)^2\\right) \\\\ &amp;= E_{(x,y)}\\left(\\left(f(x)-E(y|x)+E(y|x)-y\\right)^2\\right) \\\\ &amp;= E_{(x,y)}\\left(\\left(f(x)-E(y|x)\\right)^2\\right) + 2E_{(x,y)}\\left({\\color{red} \\left(f(x)-E(y|x)\\right)}\\left(E(y|x)-y\\right)\\right) \\\\  &amp;\\quad + E_{(x,y)}\\left(\\left(E(y|x)-y\\right)^2\\right) \\\\ &amp;= {\\color{green} E_{(x,y)}\\left(\\left(f(x)-E(y|x)\\right)^2\\right)} + {\\color{blue} E_{(x,y)}\\left(\\left(E(y|x)-y\\right)^2\\right)}, \\end{aligned} \\] <p>where the last equation is from the equivalent form of the Key Property regarding the above \\(\\color{red}{\\text{red}}\\) term as a function of \\(x\\). Note that both the \\(\\color{green}{\\text{green}}\\) and \\(\\color{blue}{\\text{blue}}\\) term are non-negative, and the \\(\\color{blue}{\\text{blue}}\\) term is independent of \\(f\\). Therefore, to minimize \\(R(f,P)\\), it is sufficient to minimize the \\(\\color{green}{\\text{green}}\\) term, and the minimizer \\(f^*(x) = E(y|x)\\).</p>"},{"location":"notes/lecture_notes/stat541_week2/#geometric-illustration","title":"Geometric Illustration","text":"<p>Pythagorean Decomposition of \\(R(f,P)\\): \\(E(y|x)\\) can be viewed as an orthogonal projection of \\(y\\) on the space of random variables that are functions of \\(x\\). </p> <p>To illustrate this decomposition, consider the space of all r.v. with finite variance(1) and the set of all r.v. that are functions of \\(x\\) becomes a hyperplane lying in this space. Assume \\(y\\) cannot simply written as certain function of \\(x\\). The Pythagorean decomposition of \\(R(f,P)\\) is shown in the following figure. </p> <ol> <li>This space is a \\(L^2\\) space. </li> </ol> <p></p>"},{"location":"notes/lecture_notes/stat541_week2/#an-example-of-decomposition","title":"An Example of Decomposition","text":"<p>If taking \\(f(x)=E(y)\\), a constant, in the decomposition of the prediction error, we have</p> \\[ \\begin{aligned}     Var(y)     &amp;= E_{(x,y)}\\left(\\left(y-E(y)\\right)^2\\right) \\\\     &amp;= E_{(x,y)}\\left(\\left(y-E(y|x)\\right)^2\\right) + E_{(x,y)}\\left(\\left(E(y|x)-E(y)\\right)^2\\right) \\\\     &amp;= Var\\left(E(y|x)\\right) + E\\left(Var(y|x)\\right).  \\end{aligned} \\] <p>The last equation is because \\(E\\left(E(y|x)\\right) = E(y)\\), by Key Property. </p>"},{"location":"notes/lecture_notes/stat541_week2/#bias-variance-decomposition","title":"Bias-Variance Decomposition","text":"<p>Denote the data set as \\(\\mathcal{D}_n\\) and the model based on learning algorithm \\(\\phi_n\\) and this data set as \\(\\hat{f}_{\\mathcal{D}_n}(x)\\) (1). Consider the expected risk of the learning algorithm \\(\\phi_n\\), denoted as \\(\\mathcal{R}\\), </p> <ol> <li>The model is the function \\(f\\) we are looking for, which uses features to predict outputs. For details, see Learning Algorithm. </li> </ol> \\[ \\begin{aligned}     \\mathcal{R}(\\phi_n,P)     &amp;= E_{\\mathcal{D}_n}\\left(R\\left(\\hat{f}_{\\mathcal{D}_n},P\\right)\\right) \\\\     &amp;= E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - y\\right)^2\\right)\\right) \\\\     &amp;= {\\color{red} E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - E(y|x)\\right)^2\\right)\\right)} + {\\color{green} E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(y - E(y|x)\\right)^2\\right)\\right)},  \\end{aligned} \\] <p>where the \\({\\color{green} \\text{green}}\\) term is the oracle predict error \\(R^*(P)\\) and we will focus on the \\({\\color{red} \\text{red}}\\) term. </p> <p>We can apply Fubinis's theorem to the \\({\\color{red} \\text{red}}\\) term. To illustrate this, we consider the following integral: </p> \\[ \\begin{aligned}     &amp;E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(y - E(y|x)\\right)^2\\right)\\right) \\\\     &amp;\\quad = \\left(\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}}\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}}\\cdots\\right) \\left(\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}} \\left(\\hat{f}_{\\mathcal{D}_n} - E(y|x)\\right)^2 p(x,y)\\,\\mathrm{d}x\\mathrm{d}y\\right) \\prod_{i=1}^{n}p(x^{(i)},y^{(i)})\\,\\mathrm{d}x^{(1)}\\mathrm{d}y^{(1)}\\cdots\\mathrm{d}x^{(n)}\\mathrm{d}y^{(n)},   \\end{aligned} \\] <p>where we can exchange the integrals by Fubinis's theorem and finally get the integral of \\(x,y\\) to be the most outside integral. Then, we have </p> \\[ \\begin{aligned}     &amp; E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(y - E(y|x)\\right)^2\\right)\\right) \\\\     &amp;\\quad = E_{(x,y)}\\left(E_{\\mathcal{D}_n}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - E(y|x)\\right)^2\\right)\\right)\\\\     &amp;\\quad = E_{(x,y)}\\left(E_{\\mathcal{D}_n}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right)\\right)^2\\right)\\right) + E_{(x,y)}\\left(E_{\\mathcal{D}_n}\\left(\\left(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right) - E(y|x)\\right)^2\\right)\\right) \\\\     &amp;\\quad = {\\color{blue} E_{(x,y)}\\left(Var_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}(x)\\right)\\right)} + {\\color{gray} E_{(x,y)}\\left(\\left(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right) - E(y|x)\\right)^2\\right)},  \\end{aligned} \\] <p>where the two terms are \\({\\color{blue} \\text{variance term}}\\) (at a fixed \\(x\\)) and \\({\\color{gray} \\text{bias term}}\\), respectively. </p> <p>To illustrate this, if the oracle predictor \\(E(y|x)\\) is some 3-degree polynomial, assume we fit data sets via linear functions and 3-degree polynomials. For the linear learning algorithms, since \\(E(y|x)\\) is some 3-degree polynomial and \\(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right)\\) is some linear function, the \\({\\color{gray} \\text{bias term}}\\) is likely to be large; on the other hand, \\({\\color{blue} \\text{variance term}}\\) may not be too large, for linear functions vary slightly compared among linear functions. This 'small variance' is shown in the following figure,    where the gray thick curve is \\(E(y|x)\\), and  colored curves are different prediction functions based on different data sets, and dashed curve is \\(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right)\\). Conversely, for 3-degree-polynomial learning algorithms, the \\({\\color{gray} \\text{bias term}}\\) is likely to be small and \\({\\color{blue} \\text{variance term}}\\) may be large. This 'large variance' is shown in the following figure.  </p> <p>To better understand this, we can consider an extreme example. If we use very high degree polynomials to fit data sets, we can make our prediction function exactly go through each training data point. In this case, the bias would be small, which mainly comes from the noise of the data set itself. However, higher degree polynomials can oscillate more freely than lower degree polynomials, and this usually leads to our prediction function shaping drastically different to each other when coming from different data sets. </p>"},{"location":"notes/lecture_notes/stat541_week3/","title":"Week3","text":""},{"location":"notes/lecture_notes/stat541_week3/#recap-on-singular-value-decomposition","title":"Recap on Singular Value Decomposition","text":""},{"location":"notes/lecture_notes/stat541_week3/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>Let \\(V\\in \\mathbb{R}^{p\\times k}\\), given by</p> \\[ V = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ v_1 &amp; \\cdots &amp; v_k \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}. \\] <p>Then \\(V\\) has orthogonal columns if</p> \\[ v_i^T v_j =\\left\\{\\begin{matrix}     &amp;  1, \\quad \\text{ if } i=j, \\\\     &amp;  0, \\quad \\text{ if } i\\neq j.  \\end{matrix}\\right. \\] <p>For such \\(V\\), we have \\(V^T V = I_k\\). Note that it is often NOT the case that \\(V V^T = I_p\\).</p> <p>A square matrix \\(V\\in \\mathbb{R}^{k\\times k}\\) with orthogonal columns is called an orthogonal matrix. Note that \\(V^{-1} = V^T\\).</p>"},{"location":"notes/lecture_notes/stat541_week3/#definition-thin-svd","title":"Definition Thin SVD","text":"<p>For a matrix \\(A\\in \\mathbb{R}^{p\\times q}\\) (\\(p\\geq q\\)), the thin SVD of \\(A\\) is a representation of the form</p> \\[ A = VDU^T, \\quad V\\in \\mathbb{R}^{p\\times q}, D\\in \\mathbb{R}^{q\\times q}, U\\in \\mathbb{R}^{q\\times q}, \\] <p>whrere \\(V\\) and \\(U\\) have orthogonal columns and \\(D\\) is diagonal.</p> <p>The column vectors of \\(V\\) and \\(U\\) are respectively called the left and right singular vectors. The singular values of \\(A\\) are the diagonal elements of \\(D\\).</p> <p>Remarkable results: Every matrix \\(A\\in \\mathbb{R}^{p\\times q}\\) has an SVD. </p>"},{"location":"notes/lecture_notes/stat541_week3/#spectral-decomposition","title":"Spectral Decomposition","text":"<p>A spectral decomposition of a symmetric matrix \\(A\\in \\mathbb{p\\times p}\\) is a representation of \\(A\\) as </p> \\[ A = VDV^T, \\quad V\\in \\mathbb{R}^{p\\times p}, D\\in \\mathbb{R}^{p\\times p}, \\] <p>where \\(V = \\begin{bmatrix}v_1 &amp; v_2 &amp; \\dots &amp; v_p \\end{bmatrix}\\) is orthogonal and \\(D = diag\\{d_1,d_2,\\dots,d_p\\}\\) is diagonal. </p> <p>The columns of \\(V\\) are the eigenvector of \\(A\\) and \\(d_i\\) is the associated eigenvalue: </p> \\[ Av_i = VDV^Tv_i = d_i v_i.  \\] <p>Remarkable results (spectral theorem): Every symmetric matrix \\(A\\in \\mathbb{R}^{p\\times p}\\) has a spectral decomposition. </p>"},{"location":"notes/lecture_notes/stat541_week3/#recap-on-multivariate-statistics","title":"Recap on Multivariate Statistics","text":""},{"location":"notes/lecture_notes/stat541_week3/#notations","title":"Notations","text":"<p>Let \\(\\boldsymbol{X}\\) be a \\(k\\)-dimensional random vector (a special case of random matrices)</p> \\[ \\boldsymbol{X}= \\begin{bmatrix}  X_1 \\\\ \\vdots \\\\ X_k \\end{bmatrix}, \\] <p>or a \\(p\\times q\\) random matrix</p> \\[ \\boldsymbol{X}= \\begin{bmatrix} X_{11} &amp;\\cdots &amp; X_{1q} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ X_{p1} &amp; \\cdots &amp; X_{pq} \\end{bmatrix}, \\] <p>where each \\(X_i\\in\\) or \\(X_{ij}\\) is a random variable. \\(E(\\boldsymbol{X})\\) is defined componentwise, i.e.</p> \\[ E(\\boldsymbol{X})= \\begin{bmatrix} E(X_{11}) &amp;\\cdots &amp; E(X_{1q}) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ E(X_{p1}) &amp; \\cdots &amp; E(X_{pq}) \\end{bmatrix}, \\] <p>and \\(E(\\boldsymbol{X})\\) has linearity: for constant matrices \\(A\\in\\mathbb{R}^{d\\times p}\\), $B\\in\\mathbb{R}^{q\\times q}, \\(C\\in\\mathbb{R}^{d\\times q}\\),</p> \\[ E(A\\boldsymbol{X}B + C) = AE(\\boldsymbol{X})B + C.  \\] <p>For random vectors \\(\\boldsymbol{X}\\in \\mathbb{R}^p\\), its covariance matrix is defined as the \\(p\\times p\\) symmetric matrix \\(Cov(\\boldsymbol{X})\\), given by</p> \\[ [Cov(\\boldsymbol{X})]_{ij} = Cov(X_i,X_j).  \\] <p>Similar to the covariance of random variable, we have</p> \\[ \\begin{aligned}     Cov(\\boldsymbol{X})      &amp;= E\\left(\\left(\\boldsymbol{X}-E(\\boldsymbol{X})\\right)\\left(\\boldsymbol{X}-E(v)\\right)^T\\right) \\\\     &amp;= E(\\boldsymbol{X} \\boldsymbol{X}^T) - E(\\boldsymbol{X})E(\\boldsymbol{X})^T.   \\end{aligned} \\] <p>From the above, we have </p> \\[ Cov(A\\boldsymbol{X}) = A \\cdot Cov(X) \\cdot A^T.  \\]"},{"location":"notes/lecture_notes/stat541_week3/#multivariate-normal-distribution","title":"Multivariate Normal Distribution","text":"<p>The multivariate normal distribution of a \\(k\\)-dimensional random vector \\(\\boldsymbol{X}=\\left(X_1, \\ldots, X_k\\right)^{T}\\) can be written in the following notation:</p> \\[ \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\quad \\text{ or } \\quad \\boldsymbol{X} \\sim \\mathcal{N}_k(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),  \\] <p>with \\(k\\)-dimensional mean vector</p> \\[ \\boldsymbol{\\mu}=E(\\boldsymbol{X})=\\begin{bmatrix} E\\left(X_1\\right) \\\\ E\\left(X_2\\right)  \\\\ \\vdots \\\\ E\\left(X_k\\right) \\end{bmatrix} \\] <p>and \\(k \\times k\\) covariance matrix</p> \\[ \\Sigma_{i, j}=E\\left(\\left(X_i-\\mu_i\\right)\\left(X_j-\\mu_j\\right)\\right)=Cov\\left(X_i, X_j\\right), \\quad \\forall i,j = 1,\\dots, k. \\] <p>\\(\\boldsymbol{\\Sigma}\\) is assumed to be positive definite (i.e. non-degenerate) and therefore, \\(\\boldsymbol{\\Sigma}^{-1}\\) is also positive definite. In this case, the density of \\(\\boldsymbol{X}\\) is given by</p> \\[ p(\\boldsymbol{z}) = \\frac{1}{\\sqrt{(2\\pi)^k\\det(\\boldsymbol{\\Sigma})}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right).  \\] <p>Fact: for a full-rank matrix \\(A\\in \\mathbb{R}^{p\\times q}\\) (\\(q\\leq p\\)) and \\(b\\in\\mathbb{R}^q\\), we have </p> \\[ A\\boldsymbol{X} + b\\sim \\mathcal{N}_k(A\\boldsymbol{\\mu}+b, A\\boldsymbol{\\Sigma}A^T).  \\]"},{"location":"notes/lecture_notes/stat541_week3/#linear-regression","title":"Linear Regression","text":"<p>The basic idea of linear regression is to assume that </p> \\[ y \\approx \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i, \\] <p>where \\(\\boldsymbol{X} = [x_1, x_2, \\dots, x_p]^T\\) (for now we assume \\(\\boldsymbol{x}\\in \\mathbb{R}^p\\)). More precisely, we make a statement about \\(p(y\\mid\\boldsymbol{x})\\) as follows: </p> \\[ y = \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i + \\varepsilon,  \\] <p>where \\(\\varepsilon\\) is noise with \\(\\varepsilon \\sim \\mathcal{N}({0,\\sigma^2})\\) and \\(\\boldsymbol{x}\\) is independent of the noise. Thus, we have </p> \\[ y\\mid \\boldsymbol{x} \\sim \\mathcal{N}(\\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i, \\sigma^2). \\] <p>We know that (under squared error loss) the oracle predictor is </p> \\[ E(y\\mid\\boldsymbol{x}) = \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i = f^*(\\boldsymbol{x}).  \\] <p>The goal is to find \\(\\beta_0,\\dots,\\beta_p\\) and thus \\(f^*\\). </p>"},{"location":"notes/lecture_notes/stat541_week3/#rewrite-training-data-in-matrix-form","title":"Rewrite Training Data in Matrix Form","text":"<p>Assume training data \\(\\{(\\boldsymbol{x}^{(i)},y^{(i)})\\}_{i=1}^n\\) were i.i.d. generated via the statement about \\(p(y\\mid\\boldsymbol{x})\\). To simplify the notations, we define </p> \\[ \\boldsymbol{Y} = \\begin{bmatrix}  y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{bmatrix}, \\quad  \\boldsymbol{\\varepsilon} = \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\vdots  \\\\  \\varepsilon^{(n)} \\end{bmatrix}, \\quad  \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta^{(0)} \\\\ \\vdots \\\\ \\beta^{(n)} \\end{bmatrix}, \\] <p>and design matrix \\(\\boldsymbol{X}\\in \\mathbb{R}^{n\\times(p+1)}\\) given by </p> \\[ \\boldsymbol{X} = \\begin{bmatrix} 1 &amp; \\left(\\boldsymbol{x}^{(1)}\\right)^T \\\\ 1 &amp; \\left(\\boldsymbol{x}^{(2)}\\right)^T \\\\ \\qquad \\vdots \\\\ 1 &amp; \\left(\\boldsymbol{x}^{(n)}\\right)^T  \\end{bmatrix}.  \\] <p>Then training data can written as </p> \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.  \\] <p>Now we want to use training data to estimate \\(\\beta_0,\\dots,\\beta_p\\) and thus \\(f^*\\). </p>"},{"location":"notes/lecture_notes/stat541_week3/#recap-on-likelihood-function","title":"Recap on Likelihood Function","text":"<p>The likelihood function \\(L(\\theta\\mid \\boldsymbol{x})\\) illustrates the probability of \\(\\theta\\) given data set \\(\\boldsymbol{x} = (x_1,\\dots,x_n)^T\\). </p> <p>More precisely, assume we have \\(n\\) samples \\((x_1,\\dots,x_n)\\) observed from a distribution \\(p_{\\theta}(x)\\) with an unknown parameter \\(\\theta\\), and our goal is to estimate \\(\\theta\\) using the observed data. </p> <p>We view the observed samples are realizations of some random variables \\(X_1, X_2, \\dots, X_n\\), which has a joint density function \\(p\\left(X_1, \\dots, X_n \\mid \\theta\\right)\\). Given \\(X_1=x_1, \\dots, X_n=x_n\\), we may consider the probability of this event being observed, which is the likelihood function (a function of \\(\\theta\\)) defined by:</p> \\[ L(\\theta)=L\\left(\\theta \\mid x_1, \\dots, x_n\\right)=p\\left(X_1=x_1, \\dots, X_n=x_n \\mid \\theta\\right).  \\] <p>Note that the likelihood function is NOT a probability density function. It measures the support provided by the data for each possible value of the parameter. If we compare the likelihood function at two parameter points and find that </p> \\[ L\\left(\\theta_1 \\mid \\boldsymbol{x}\\right)&gt;L\\left(\\theta_2 \\mid \\boldsymbol{x}\\right) \\] <p>then the sample we actually observed is more likely to have occurred if \\(\\theta=\\theta_1\\) than if \\(\\theta=\\theta_2\\). This can be interpreted as \\(\\theta_1\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_2\\). Therefore, one approach to estimate \\(\\theta\\) is to choose the value of \\(\\theta\\) which gives you the highest probability among all possible values. </p> <p>With the assumption that samples are drawn i.i.d., the joint probability is given by multiplied probabilities, i.e.  </p> \\[ p\\left(X_1, \\dots, X_n \\mid \\theta\\right) = \\prod_{i=1}^n p_\\theta\\left(X_i\\right),  \\] <p>hence taking the log transforms this into a summation, which is usually easier to maximize analytically. Thus, we often write the log-likelihood rather than the likelihood. </p>"},{"location":"notes/lecture_notes/stat541_week3/#estimate-the-coefficients-beta_i","title":"Estimate the Coefficients \\(\\beta_i\\)","text":"<p>Here we find an estimate of \\(\\boldsymbol{\\beta}\\) by maximum likelihood estimation: </p> <p>By \\(\\boldsymbol{X}\\) is independent of the noise (and vice versa), we have </p> \\[ \\boldsymbol{Y} \\mid \\boldsymbol{X} \\sim \\mathcal{N}_{n}(\\boldsymbol{X}\\boldsymbol{\\beta}, \\sigma^2I_n), \\] <p>and thus, the (log-)likelihood function is given by the density function of the above multivariate normal distribution. </p> <p>By maximizing the likelihood function, we can estimate \\(\\boldsymbol{\\beta}\\): </p> \\[ \\begin{aligned} \\hat{\\boldsymbol{\\beta}} &amp;= \\operatorname*{arg\\, max}_\\beta \\ln(p(\\boldsymbol{Y} \\mid \\boldsymbol{X})) \\\\ &amp;= \\operatorname*{arg\\, max}_\\beta \\left(\\frac{1}{2\\sigma^2} (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta})^T (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta})\\right) \\\\ &amp;= \\operatorname*{arg\\, max}_\\beta \\|\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|^2, \\end{aligned} \\] <p>which becomes a least squares problem. We take a gradient with respect to \\(\\boldsymbol{\\beta}\\) and set the gradient for 0, and after solving for \\(\\boldsymbol{\\beta}\\), we have </p> \\[ \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y},  \\] <p>where we assume \\(\\boldsymbol{X}\\) is a full-rank matrix and thus \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) has an inverse. </p> <p>Furthermore, we can find the distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) conditional on the training features \\(\\boldsymbol{X}\\): </p> \\[ E(\\hat{\\boldsymbol{\\beta}}\\mid \\boldsymbol{X}) = E\\left(\\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y}\\mid \\boldsymbol{X}\\right) = \\boldsymbol{\\beta},  \\] <p>which indicates that \\(\\hat{\\boldsymbol{\\beta}}\\) is unbiased; </p> \\[ Cov(\\hat{\\boldsymbol{\\beta}}\\mid \\boldsymbol{X}) = Cov\\left(\\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y}\\mid \\boldsymbol{X}\\right) = \\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}; \\] <p>and therefore, </p> \\[ \\hat{\\boldsymbol{\\beta}}\\mid \\boldsymbol{X} \\sim \\mathcal{N}_{p+1}(\\boldsymbol{\\beta}, \\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}).  \\]"},{"location":"notes/lecture_notes/stat541_week3/#evaluate-ols-prediction-estimate","title":"Evaluate OLS Prediction Estimate","text":"<p>The above estimation is called ordinary least squares (OLS) in statistics. Now we evaluate our estimation when receiving a new data pair \\((\\boldsymbol{x_{*}},y_*)\\). We make our prediction by</p> \\[ \\hat{f}(\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T \\hat{\\boldsymbol{\\beta}}, \\quad \\tilde{\\boldsymbol{x}} = \\begin{bmatrix} 1 \\\\ \\boldsymbol{x_{*}} \\end{bmatrix}. \\] <p>Consider the bias of \\(\\hat{f}\\) (conditional on \\(\\boldsymbol{X},\\boldsymbol{x_{*}}\\)), </p> \\[ E(\\hat{f}(\\boldsymbol{x_{*}}) \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T E(\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}).  \\] <p>We know that \\(E(y_* \\mid \\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T \\hat{\\boldsymbol{\\beta}}\\),and thus the OLS prediction is unbiased(1). </p> <ol> <li>This conclusion relied on the assumption that the data has a linear relationship. In practice, our model is only an approximation to the true distribution. So we will have bias. </li> </ol> <p>Now, we consider the variance:</p> \\[ Var(\\hat{f}(\\boldsymbol{x_{*}}) \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T\\cdot Var(\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) \\tilde{\\boldsymbol{x}} = \\sigma^2\\tilde{\\boldsymbol{x}}^T(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\tilde{\\boldsymbol{x}}.  \\] <p>Roughly speaking, if \\(\\boldsymbol{x_{*}}\\) is similar to the training data features, the variance will be small; otherwise, the variance will be large. </p>"},{"location":"notes/lecture_notes/stat541_week3/#interval-estimate","title":"Interval Estimate","text":"<p>We want to make a prediction interval for \\(y_*\\) in the form of  </p> \\[ \\left[C_{low}(\\boldsymbol{Y}, \\boldsymbol{X}, \\boldsymbol{x_{*}}),C_{high}(\\boldsymbol{Y}, \\boldsymbol{X}, \\boldsymbol{x_{*}})\\right].  \\] <p>We hope \\(y_*\\) to be contained in this with probability of at least \\(1-\\alpha\\) (usually \\(\\alpha = 0.05\\) or \\(\\alpha = 001\\)), i.e. </p> \\[ Pr\\left(y_*\\in \\left[C_{low},C_{high}\\right] \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) \\geq 1-\\alpha.  \\]"},{"location":"suggestions/acrobatbookmark/","title":"Acrobat\u81ea\u52a8\u6dfb\u52a0\u4e66\u7b7e","text":""},{"location":"suggestions/acrobatbookmark/#_1","title":"\u51c6\u5907","text":"<p>1\u3001Adobe Acrobat 2\u3001AutoBookmark Standard Plug-in\uff1aadobe acrobat\u7684\u81ea\u52a8\u751f\u6210\u4e66\u7b7e\u7684\u63d2\u4ef6(1)</p> <ol> <li>\u89e3\u538b\u540e\u5c06\u76ee\u5f55\u62f7\u8d1d\u5230Adobe\\AcrobatDC\\Acrobat\\plug_ins\u6587\u4ef6\u5939\u4e0b </li> </ol>"},{"location":"suggestions/acrobatbookmark/#_2","title":"\u6d41\u7a0b","text":""},{"location":"suggestions/acrobatbookmark/#1pdfword","title":"1\u3001\u5bfc\u51faPDF\u76ee\u5f55\u5185\u5bb9\u5230Word","text":"<p>\u5c06word \u5185\u5bb9\u62f7\u8d1d\u5230\u65b0\u5efa\u7684txt\u6587\u672c\u4e2d(TXT\u9700\u8981\u4f7f\u7528ANSI\u7f16\u7801(1))</p> <ol> <li>\u7528\u8bb0\u4e8b\u672c\u6253\u5f00txt\u6587\u4ef6\uff0c\u9009\u62e9\u53e6\u5b58\u4e3a\uff0c\u7f16\u7801\u4fee\u6539\u4e3aANSI </li> </ol>"},{"location":"suggestions/acrobatbookmark/#2txt","title":"2\u3001TXT\u6587\u4ef6\u683c\u5f0f\u7f16\u8f91","text":"<p>\u6700\u540eTXT\u6587\u4ef6\u5185\u5bb9\u683c\u5f0f\uff1a</p> <p>[\u6807\u7b7e\u540d\u79f0],[\u76ee\u5f55\u9875\u7801],[\u76ee\u5f55\u9875\u7801\u548c\u5b9e\u9645\u9875\u7801\u7684\u504f\u79fb\u91cf]</p> <p></p> <p>\u5904\u7406\u6d41\u7a0b\uff1a</p> <ol> <li>tab\u66ff\u6362\u4e3a\u7a7a\u683c</li> <li>\u591a\u4e2a\u7a7a\u683c\u66ff\u6362\u4e3a\u4e00\u4e2a\u7a7a\u683c</li> <li>\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u4fee\u6539\u6bcf\u884c\u7684\u5185\u5bb9</li> <li>\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u4fee\u6539\u5404\u7ea7\u76ee\u5f55\u7684\u7f29\u8fdb</li> </ol> <p>\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u6cd5\uff1a</p> <p>\\w: \u6570\u5b57\u6216\u5927\u5c0f\u5199\u5b57\u6bcd +: \u4e00\u6b21\u6216\u591a\u6b21\u5339\u914d\u524d\u9762\u7684\u5b57\u7b26 (): \u6355\u83b7\u5305\u542b\u5728\u62ec\u53f7\u4e2d\u7684\u8868\u8fbe\u5f0f\u5e76\u5bf9\u5176\u8fdb\u884c\u9690\u5f0f\u7f16\u53f7 $1: \u5f15\u7528\u7b2c\u4e00\u4e2a\u9690\u5f0f\u7f16\u53f7\u5185\u5bb9 e.g.: (\\w+,\\w)\\n \u66ff\u6362\u4e3a $1,+11\\n\\t</p> <p></p>"},{"location":"suggestions/acrobatbookmark/#3txt","title":"3\u3001\u5bfc\u5165TXT\uff0c\u81ea\u52a8\u751f\u6210\u4e66\u7b7e","text":"<ol> <li>\u589e\u6548\u5de5\u5177 -&gt; Bookmarks -&gt; Create From TextFile</li> <li>\u9009\u62e9TXT\u6587\u4ef6\uff0cField delimiter\u8981\u8bbe\u4e3aComma\uff0c\u70b9\u51fbOK\u540e\u81ea\u52a8\u751f\u6210\u4e66\u7b7e</li> </ol>"},{"location":"suggestions/creditcardcomp/","title":"Credit Cards Comparison","text":""}]}